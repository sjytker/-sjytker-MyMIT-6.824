





# LAB2





# LAB3





# LAB4

#### 简介

- lab4 要求数据库分库，以提高访问速度，因为如果只有一个 cluster，那么 client 请求越来越多时，leader 的队伍会非常长，client 的请求时延就会很高
- 解决办法是，把不同范围的 key 哈希到不同的 cluster 中，每个 cluster 都是一个基于 raft 一致性算法的集群，拥有 3 或 5 个 server，各自有一个 leader。这样就降低了请求延迟
- 分为 ShardMaster 和 ShardKV 两个模块，ShardMaster 负责解决加入和移除 cluster 时 Shard (子库) 的重新分配；ShardKV 解决 k-v 数据库的分库请求



### ShardMaster

#### 注意点

- master 和 group 都是有 3、5 个 server 的集群。其中 master 像 lab3 实现 k-v service 一样实现 client, server。log 有 join, leave, move, query 四种不同操作的命令，分别是加入 cluster，移除 []gids 的 cluster，手动更改 shard 的 gid，查询某个 index 的 config
- ShardMaster 不需 persist log，因为 shardMaster 是对 cluster 的历史操作，即使 offline 了，现有的 cluster configuration 依然是没有改变的。
- 需要注意 golang RPC 传输 interface 参数时，需要用 gob.register 注册这个 interface 可能出现的类型。可以把这些 register 放在包初始化函数 init() 中

#### 关键设计

- 重点关注 join 和 leave 两个操作，其中 join 使当前 config group 长度变大，avg = NShard / len(config.groups) 小于等于 lastConfig 。而 leave 与之相反
- 由于 lab4 要求增加或删除 cluster 时，Shards 变动尽可能小，因此不能暴力把 lastConfig 的 Shards 清空，然后重新分配，因为这样使得新旧 config 重合部分的 Shards 也重分配了，而它们实际上不需要被重分配。
- 新旧 config 可以如下图示：  ###....x1....||.....x2....###......x3.....||     其中 x2 是重合部分。不妨设 x1, x2 是新 config， x2, x3 是旧 config
- 先求出 avg = NShards / len(config.Group),   left = NShard % len(config.Group)
  1. 当 curLen < lastLen，也就是 leave 时，我们从 x3 分部分 shards 到 x1, x2 中，其中 x2 可能不需要被增加。遍历所有 config.Groups。 r = avg - lastCnt[gid]  。FIRST,  move x3 to x2，注意此时要检查两次，如果 left > 0，分配多余的 1 之前和之后 r 是否为 0，如果是 0 就不用调整了。 SECOND，move x3 to x1。
  2. 当 curLen >= lastLen，也就是 join 时，每个新 Group 的 Shard <= 每个旧 Group 的 Shard。可以用和上一种情况同样的思路做，但这里我采取了另一种不同的思路：先把 x2 多余部分以及 x3 的 config.Shard[i] = 0，清零后再把它们统一分配到 x1 中。
- ShardMaster 有三种状态，STABLE, TRANSITION, FREEZE。其中只有 STABLE 状态可以执行 join, leave, move。 TRANSITION 为 RAFT 正复制新的 configuration，FREEZE 为同步完毕，等待一段时间让 ShardKV 也进行数据迁移。



### ShardKV

- 用一个 go routine 不断询问 ShardMaster 最新的 configuration
- Get 和 PutAppend 时，区分 ErrWrongLeader 和 ErrWrongGroup 两种错误，前者在同 group 换 server 发送，后者换 group 发送
- 与非分库的 k-v service 实现相比，非分库只用一个 map[string]string 结果存储数据，分库时需要每个 Shard 作为 key 保存一个 map，也就是 map[int]map[string]string
- shardMaster 更新 configuration 后，ShardKV 需要立马跟进吗？好像不影响对 client 提供服务，晚一点让 findConfig() 跟进也没关系。但可能会产生并发问题，如果 server 收到了 get 请求，此时又刚好发生 data migration，把这个 get 对应的 shard 删除了，那么最后会 get 到错误的空字符串。因此 migration 一但开始了就不能接受其他 get, put 请求。

  

#### 关键设计

- ShardKV serivce 的关键问题是，在 ShardMaster 改变 configuration 时，如何把对应子库 shard 的 k-v 数据迁移到另一个 gid？      
- 我的解决办法是，在 ShardKV server 收到 get, put 请求时，sever 发给 ShardMaster 判断当前 shard 是否在我这个 gid，如果已经不在了，那么让 ShardKV client 遍历所有 gid，找到所在的一个。然后把 oldGid 这个 Shard 对应的 data 发往 newGid  
- 新问题：如果按上述设计，只能解决哈希到 oldGid 时的数据迁移问题，如果一开始就哈希到 newGid 呢
- 解决办法：
  1. 先比较 lastConfig 和 curConfig 的 group 数量是否不同，如果有所改变，client 先请求 lastConfig，如果这个 key 已经执行过 migration，那么请求 curConfig
  2. Shard server 周期性 findConfig 时，如果当前 config 长度和上一个不同，那么发生了迁移。当前 gid server 的 shard 如果迁到了其他 gid，那么主动调用 RPC 把它迁过去。

- 

