<<<<<<< HEAD


## lab2 有小概率 failTest（困难)

- 在写完 lab2 后，由于存在很多随机的 timeout，所以一次测试成功 pass 不能代表永远能 pass。用一个脚本自动跑 500 次 lab2ABC test（每次有 20+ 个 test 函数)

- 经过一系列 debug，还是有小概率 failTest。500 次 fail 10 次，错误主要 fail to reach agreement ，原因是：没leader；有 leader，但 commit 的不够

- 查了几天日志，反复看了 paper 和参考别人的代码实现。我百思不得其解，明明逻辑都是对的，为什么会有 2% 概率 fail test？

- 暂时找不到突破口，我决定把日志打得更详细一些，每一个判定流程，都打个日志看看当前状态如何。

- 用 subline 打开几万行的日志，定位的关键的几千行之间反复看，突然注意到：有时候选不出 leader，不是由于 election 或 replication 的逻辑错误，而是网络发送的包延迟到达了，超过了程序设定的 RPCTimeout 参数，自动结束了 election。因此 candidate 没有收到 follower 的投票，导致一直选不出 leader。

- fail to reach agreement 错误，也是同样情况，AE match fail 几次，并且网络发包屡屡延时到达，导致出现不该出现的 election timeout，然后出现复杂的奇怪的问题。

- 值得注意的是，这个 test 的 golang 代码并非真的通过网络发送 RPC，而是在本地模拟。所以它不是消耗网络资源的，而是消耗 CPU

- 于是我把 RPCTimeout, beatPeriod 等参数都调了一下，虽然不同参数下 fail 概率不同，但都会 fail，始终没找到怎么解决。

  

- 最终发现了两个问题，都是 Raft 论文中没有提到的：

  1. 参考他人代码发现：选主逻辑没有完全写对：如果 args.term == rf.currentTerm，且 voteFor != -1 ，那么这个 follower 曾经投给了其他人，他就不再投票了，即使 candidate 的 log 非常新。这时候根据 voteFor 是否等于 candidateId 返回 true or false。

  2. 看课程 student guide 发现：乱序到达的 AE，我没有特殊处理。有时候 leader 发的旧 AE 堵塞在网络中，但是有新 leader 的新log 已到达，那么如果旧 AE 覆盖了新 AE，那么 leader 已经 applied 到后面了，并且认为 follower 也 applied 到了后面，但是事实上这个 server 并没有 applied 到后面。

     这在测试中会报 fail to reach agreement 错误。

- 显然，这两种情况没有论文可依，如果光凭自己 YY ，很难想到代码没覆盖到什么情况。毕竟 Raft 论文是 Standford PHD 的大作，而我在分布式领域经验不多，要是纯凭一己之力解决所有这些问题实在太难了。

  





## Client 请求死锁

如果对整个 apply() 函数加锁，可能会死锁，原因如下：

- for isLeader{} 的循环中，每次 apply timeout 都要重新判断当前是否 leader，而调用 rf.getState() 也要加锁
- 但注意，如果此时线程调度到 rf leader，并且 msg 已经放入了 apply channel，那么这个 rf leader 将带 mutex 阻塞在 channel，kvserver 想通过 rf.getState() 判断是否还是 leader，将永远获取不到锁。

#### 解决方法：

* 不要拿着 mutex 等 channel
* 不要拿着 mutex 等 channel
* 不要拿着 mutex 等 channel
* 把临界变量加锁取出来，解锁。再 block in channel





## kvserver 提交 cmd 的问题

#### **问题**： 

​	起初，我在 kvserver putAppend 或 get 后，阻塞在 rf.applyChan 监听提交，但是这有一个问题。如果在 applyTimeout 内，deposed leader 被 AE 刷新成 follower，然后在 applyChan apply 了新的 log，那么这个 kvserver 从 applyChan 读取到的 log 将和前面 start() 的 log 不符。

#### 解决：

1. 给 log 加入 unique requestID，每次从 channel 中收到消息时，检查这个消息的 requestID 是否和自己发出去的一致。

2.  不可以让 KVServer 阻塞在 apply chan，因为选出新 leader 时，会重新提交以前已经提交过的 log。如果阻塞在 apply chan，就大概率会收到和当前 RequestID 不一致的 log，然后返回 ErrNoLeader 错误。

   正确的做法：

   - 创建一个独立的 waitApplyCh 函数来阻塞监听 ApplyChan
   - 创建一个 NotifyMsgChan 来监听当前 RequestID 是否被被 ApplyChan 提交，如果是则往 NotifyMsgChan  推消息，否则不理。
   - 设置一个 Timeout，超出此事件还没受到 NotifyMsgChan 则中断等待。此时可能是这个 server 处于 minority partition 中，导致一直无法将命令 commit。应在 client 程序重新寻找 leader。















---

> #### 其他小问题





## 日志太大，很难定位错误

- 在写第二个任务，KVservice 的时候，日志已经涨到 50-100 MB。而且 KVservice 和 Raft 存在不小的耦合度，导致每次测试出错时都很难定位，到底出了什么问题。每次找一个小错误，都得花上半天甚至超过一天。

  

#### 解决：

- 可以逐个函数或模块看，是否出现了行为异常
- 可以对总体流程作二分，先查中间步骤是否出错，如果没有则查后 1 / 2，如果有则查前 1 / 2……







=======
>>>>>>> edacab21560e1960d239d963a1287729ab342ea2
## json.NewEncoder 无法写入文件

**解决**：

​	struct 可导出性的问题，必须 uppercase 开头的变量才是可导出的，才能被写入 json 文件



## rpc 调用找不到 master 的函数

**问题**：

​	我在 master 写了 2 个 rpc 调用函数，其中一个能正常调用。另一个报错找不到该方法。调用方式完全一致，并且 master 也确定有 worker 调用的同名函数。

​	（搞了半天才找到问题）

**解决**：

​	还是 Golang exported 类型的问题。无论是 type，还是参数，都只有 uppercase 开头才是 exported，否则 unexported。

​	在 unexported 的情况下，文件不能写入，rpc 也无法调用。



## Mutex 的问题

<<<<<<< HEAD
​	Java 的 lock, synchronized 是可重入锁，而 golang 的 mutex 是不可重入的。C ++ 也不可重入



## 共享变量问题

​	由于每个 Raft 对象既发送，又接收，它的对象变量是共享的，所有访问到共享变量的函数都应该加锁。否则会出现并发问题。

​	但是锁粒度不宜太大，也就是说，不需要整个函数都加锁，而是读取和修改共享变量时加锁，完毕后释放。
=======
**问题***：

​	习惯了 Java 的可重入锁，没意识到 golang 的 mutex 是不可重入的



## 共享变量问题

由于每个 Raft 对象既发送，又接收，它的对象变量是共享的，所有访问到共享变量的函数都应该加锁。否则会出现并发问题。
>>>>>>> edacab21560e1960d239d963a1287729ab342ea2



## Election 忙等

​	一开始我用 waitGroup（效果与 Java CountDownLatch 相同）等待所有 server 返回 RequestVote 结果。忽略了如果此时有 server down 无法选出 leader 的问题。

<<<<<<< HEAD
​	加上 time.Timer 来计时，timeout 则退出。设置常量 RPCTimeout


=======
​	加上 time.Timer 来计时，timeout 则退出



## TestB 偶尔出现 fail to reach agreement

- 这个错误捣鼓了几天。有时出现报错，有时又能 pass all test
- 最大的困难是，问题不可重现。多个测试函数都有概率出现。
- 最后，通过阅读所有测试代码 config 类，了解测试流程，得知是 RequestVote 选 leader 的逻辑没有写对



## TestB 经常出现 applied error

- 检查了很久代码逻辑，发现 raft 算法的 RequestVote 和 AppendEntries 应该都没有写错
- 注意到这个报错的函数 command list 很长，再结合阅读测试代码，猜测应该是 AE 逐步回退太慢了。遂采用优化的，低时间复杂度的方法



## 有低概率 failTest（hard)

- 由于存在很多随机的 timeout，所以一次测试成功 pass 不能代表永远能 pass。
- 用一个脚本自动跑 100 次 lab2ABC test（共 >20 个不同情况的 test，每做完所有算 1 次），30% 概率 fail，这些 fail 的日志很奇怪，看上去附近的代码没逻辑错误，但打出的日志仔细看规律，其实并不对。找了很久，发现同时存在两个 leader。导致测试脚本有时把 cmd 放到了 leader1，有时放到了 leader2
- 这种情况可能存在于：第一个 leader down 了，过了一段时间后 rejoin，此时有两个 leader，但是我在 AE 中，只有 reply.success = true 才让接受者强制转为 follower。改为任何情况下都转为 follower。
- 更改后，还是有极低概率 failTest。并且在本机 (4 核 16 G) 5% 概率 fail，服务器 (10 核 64 G) 1% 概率 fail。有时候日志会出现 no leader 的错误，猜测是本机资源短缺，造成多次 split vote 选不出 leader，超出了测试时间。
- 根据 Raft paper 作者所述，实际中不会经历这么频繁且恶劣的 server down，所以有些优化甚至意义不大。总的来说，100次恶劣测试失败 1 次的概率，也算可以了。
>>>>>>> edacab21560e1960d239d963a1287729ab342ea2









