​	

## lab2 有小概率 failTest（困难)

- 在写完 lab2 后，由于存在很多随机的 timeout，所以一次测试成功 pass 不能代表永远能 pass。用一个脚本自动跑 500 次 lab2ABC test（每次有 20+ 个 test 函数)

- 经过一系列 debug，还是有小概率 failTest。500 次 fail 10 次，错误主要 fail to reach agreement ，原因是：没leader；有 leader，但 commit 的不够

- 查了几天日志，反复看了 paper 和参考别人的代码实现。我百思不得其解，明明逻辑都是对的，为什么会有 2% 概率 fail test？

- 暂时找不到突破口，我决定把日志打得更详细一些，每一个判定流程，都打个日志看看当前状态如何。

- 用 subline 打开几万行的日志，定位的关键的几千行之间反复看，突然注意到：有时候选不出 leader，不是由于 election 或 replication 的逻辑错误，而是网络发送的包延迟到达了，超过了程序设定的 RPCTimeout 参数，自动结束了 election。因此 candidate 没有收到 follower 的投票，导致一直选不出 leader。

- fail to reach agreement 错误，也是同样情况，AE match fail 几次，并且网络发包屡屡延时到达，导致出现不该出现的 election timeout，然后出现复杂的奇怪的问题。

- 值得注意的是，这个 test 的 golang 代码并非真的通过网络发送 RPC，而是在本地模拟。所以它不是消耗网络资源的，而是消耗 CPU

- 于是我把 RPCTimeout, beatPeriod 等参数都调了一下，虽然不同参数下 fail 概率不同，但都会 fail，始终没找到怎么解决。

  

- 最终发现了两个问题，都是 Raft 论文中没有提到的：

  1. 参考他人代码发现：选主逻辑没有完全写对：如果 args.term == rf.currentTerm，且 voteFor != -1 ，那么这个 follower 曾经投给了其他人，他就不再投票了，即使 candidate 的 log 非常新。这时候根据 voteFor 是否等于 candidateId 返回 true or false。

  2. 看课程 student guide 发现：乱序到达的 AE，我没有特殊处理。有时候 leader 发的旧 AE 堵塞在网络中，但是有新 leader 的新log 已到达，那么如果旧 AE 覆盖了新 AE，那么 leader 已经 applied 到后面了，并且认为 follower 也 applied 到了后面，但是事实上这个 server 并没有 applied 到后面。

     这在测试中会报 fail to reach agreement 错误。

- 显然，这两种情况没有论文可依，如果光凭自己 YY ，很难想到代码没覆盖到什么情况。毕竟 Raft 论文是 Standford PHD 的大作，而我在分布式领域经验不多，要是纯凭一己之力解决所有这些问题实在太难了。

  





## Client 请求死锁

如果对整个 apply() 函数加锁，可能会死锁，原因如下：

- for isLeader{} 的循环中，每次 apply timeout 都要重新判断当前是否 leader，而调用 rf.getState() 也要加锁
- 但注意，如果此时线程调度到 rf leader，并且 msg 已经放入了 apply channel，那么这个 rf leader 将带 mutex 阻塞在 channel，kvserver 想通过 rf.getState() 判断是否还是 leader，将永远获取不到锁。

#### 解决方法：

* 不要拿着 mutex 等 channel
* 不要拿着 mutex 等 channel
* 不要拿着 mutex 等 channel
* 把临界变量加锁取出来，解锁。再 block in channel



#### 如何通过日志查死锁：

- 封装 lock 函数，每次 lock 都传入一个字符串。 lock 成功后，往数组中推入此字符串。
- 查阅日志，如果发生了死锁，那么某些日志输出一定是大量减少的，找到一开始减少的地方，这里大概率就死锁了。查看 lockSeq，检查最后几个加锁是否可能构成死锁。







## kvserver 提交 cmd 的问题

#### **问题**： 

​	起初，我在 kvserver putAppend 或 get 后，阻塞在 rf.applyChan 监听提交，但是这有一个问题。如果在 applyTimeout 内，deposed leader 被 AE 刷新成 follower，然后在 applyChan apply 了新的 log，那么这个 kvserver 从 applyChan 读取到的 log 将和前面 start() 的 log 不符。

#### 解决：

1. 给 log 加入 unique requestID，每次从 channel 中收到消息时，检查这个消息的 requestID 是否和自己发出去的一致。

2.  不可以让提交 cmd 的线程阻塞在 apply chan，因为如果同时提交多个请求，那么 applyCh 被放入消息时，取出消息的线程是随机的。

   正确的做法：

   - 创建一个独立的 waitApplyCh 函数来阻塞监听 ApplyChan
   - 创建一个 NotifyData Map 来记录当前 RequestID 是否需要被 ApplyChan 提交，如果收到了 ApplyCh 提交的这个 RequestId，则往 NotifyMsgChan  推消息，否则不理。
   - 设置一个 Timeout，超出此事件还没受到 NotifyMsgChan 则中断等待。此时可能是这个 server 处于 minority partition 中，导致一直无法将命令 commit。应在 client 程序重新寻找 leader。





## 等待 cmd 命令提交需要两个 uniqueId （困难）

#### 问题：

​	起初我把 client 每次提交的 op.MsgId 放在 NotifyData 哈希表里，每次 NotifyCh 收到消息后删除这个 key。但如果有两个重复提交，存在以下并发问题：

- 如果有重复提交，第一个提交已经给 notifyCh，但线程一 waitForRaft() 还没删除；此时第二个 Apply判断 NotifyData 里存在这个 MsgId key，并且阻塞在 NotifyCh。
- 这时候由于调度问题，线程二的 waitForRaft() 阻塞了，没有马上从 NotifyCh 中取出消息。现线程一获得 CPU 时间片，把这个 NotifyCh 删除。那么线程二就永远无法从这个相同的 NotifyCh 取出消息， WaitApplyCh() 函数将永远阻塞在 NotfiyCh，因此即使 Client 重复提交，Apply 线程也没有为其工作。



> ​	这个问题查了一天。花了很多时间读测试代码，定位到哪个测试语句没有通过。
>
> ​	然后花了很长时间艰难地通过日志定位到哪个函数出了问题，通过 ctrf-F 定位测试代码 print 出的关键语句，用笔把代码行记录下来，然后再思考这些关键行数之间发生了什么，为什么死锁。
>
> ​	并发情况可能很多，取决于 CPU 的调度，语句间的调度顺序都是不确定的，使得看起来正确的逻辑，可能会在某种奇怪的调度下死锁。



#### 解决：

​	解决问题的关键是，线程一删除 NotifyCh 时，所用的 key 不能影响线程二，因此他们需要分别使用不同的 key。用 MsgId 标记 client 是否重复发送，用 RequestId 标记每次发送，即使是重复发送，也应有不同的 RequestId。





## 线性不一致错误

#### 产生原因：

​	你有大量的副本，或许所有的副本看到第一个写请求，也就是写 X 为 1 的请求，但是只有部分副本看到了第二个写请求，也就是写 X 为 2 的请求。所以，当你向一个已经“拖后腿”的副本请求数据时，它仍然只有 X 的值为1。

​	对于读请求不允许返回旧的数据，只能返回最新的数据。或者说，对于读请求，线性一致系统只能返回最近一次完成的写请求写入的值。

#### 解决：

​	实际上逻辑没写对，leader 没有同步完毕，请求了旧数据





## lab4B join, leave 和 migration 并发问题

#### 问题：

​	如果现在有 G100，新增 G101，在 migration 时，新增 G102，移除 G100，那么按我的设计，检测 group len 发生变化时进行 migration，就会判断为不需要进行 migration，但此时显然 G102 是没有数据的。

#### 解决：

​	每次 join 或 leave 后 sleep 一段时间，保证 ShardKV 数据迁移后再进行下一次 reconfiguation















---

> ## 其他小问题





## 日志太大，很难定位错误

- 在写第二个任务，KVservice 的时候，日志已经涨到 50-100 MB。而且 KVservice 和 Raft 存在不小的耦合度，导致每次测试出错时都很难定位，到底出了什么问题。每次找一个小错误，都得花上半天甚至超过一天。

  

#### 解决：

- 可以逐个函数或模块看，是否出现了行为异常
- 可以对总体流程作二分，先查中间步骤是否出错，如果没有则查后 1 / 2，如果有则查前 1 / 2……







## json.NewEncoder 无法写入文件

**解决**：

​	struct 可导出性的问题，必须 uppercase 开头的变量才是可导出的，才能被写入 json 文件



## rpc 调用找不到 master 的函数

**问题**：

​	我在 master 写了 2 个 rpc 调用函数，其中一个能正常调用。另一个报错找不到该方法。调用方式完全一致，并且 master 也确定有 worker 调用的同名函数。

​	（搞了半天才找到问题）

**解决**：

​	还是 Golang exported 类型的问题。无论是 type，还是参数，都只有 uppercase 开头才是 exported，否则 unexported。

​	在 unexported 的情况下，文件不能写入，rpc 也无法调用。



## Mutex 的问题

​	Java 的 lock, synchronized 是可重入锁，而 golang 的 mutex 是不可重入的。C ++ 也不可重入



## 共享变量问题

​	由于每个 Raft 对象既发送，又接收，它的对象变量是共享的，所有访问到共享变量的函数都应该加锁。否则会出现并发问题。

​	但是锁粒度不宜太大，也就是说，不需要整个函数都加锁，而是读取和修改共享变量时加锁，完毕后释放。



## Election 忙等

​	一开始我用 waitGroup（效果与 Java CountDownLatch 相同）等待所有 server 返回 RequestVote 结果。忽略了如果此时有 server down 无法选出 leader 的问题。

​	加上 time.Timer 来计时，timeout 则退出。设置常量 RPCTimeout











